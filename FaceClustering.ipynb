{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#import basic libraries and setup dataset path\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # image processing\n#from PIL import Image # image processing\n#import lodgepole.image_tools as lit # linear approximation of gamma correction KAGGLE WILL NOT IMPORT THIS\nimport os\n\npath = '/kaggle/input' #the path of the directory that the dataset folder lies, change to run on a different machine","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-24T09:19:12.592179Z","iopub.execute_input":"2022-06-24T09:19:12.593090Z","iopub.status.idle":"2022-06-24T09:19:12.620265Z","shell.execute_reply.started":"2022-06-24T09:19:12.592989Z","shell.execute_reply":"2022-06-24T09:19:12.618951Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#randomly pick people of which there are least 50 pictures\n\nimport random\n\npeople = [] # list of randomly selected people of which there are least 50 pictures\nwhile len(people)<10:\n    r = random.randint(1, 4000)\n    _, _, files = next(os.walk(path+'/11785-spring2021-hw2p2s1-face-classification/train_data/'+str(r)))\n    file_count = len(files)\n    if file_count>=50:\n        people.append(r)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T09:19:12.622865Z","iopub.execute_input":"2022-06-24T09:19:12.623377Z","iopub.status.idle":"2022-06-24T09:19:12.768921Z","shell.execute_reply.started":"2022-06-24T09:19:12.623329Z","shell.execute_reply":"2022-06-24T09:19:12.767719Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#get 50 images of each person picked and put them in the training array\n\nfrom skimage import color\nfrom skimage import io\n# color_img = np.asarray(Image.open(img_filename)) / 255\n# gray_img = lit.rgb2gray_approx(color_img) \n#KAGGLE WONT IMPORT lit, skimage used instead below\n\ntraining = list() #contains all the images to be used (500). 50 continous of each person and in the order the people were picked\nfor p in people:\n    c=0\n    for dirname, _, filenames in os.walk(path+'/11785-spring2021-hw2p2s1-face-classification/train_data/'+str(p)):\n        for filename in filenames:\n            if c<50:\n                img = color.rgb2gray(io.imread(path+'/11785-spring2021-hw2p2s1-face-classification/train_data/'+str(p)+'/'+filename))\n                training.append(img.flatten())#images need to be flat (vectors and not arrays)\n                c+=1","metadata":{"execution":{"iopub.status.busy":"2022-06-24T09:19:12.771871Z","iopub.execute_input":"2022-06-24T09:19:12.772837Z","iopub.status.idle":"2022-06-24T09:19:15.209917Z","shell.execute_reply.started":"2022-06-24T09:19:12.772784Z","shell.execute_reply":"2022-06-24T09:19:15.207989Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"y_true = list() #indicates how training array was actually made, used for evaluation\nfor i in range(0,10):\n    for j in range(0,50):\n        y_true.append(i)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T09:19:15.211837Z","iopub.execute_input":"2022-06-24T09:19:15.212270Z","iopub.status.idle":"2022-06-24T09:19:15.224863Z","shell.execute_reply.started":"2022-06-24T09:19:15.212222Z","shell.execute_reply":"2022-06-24T09:19:15.219331Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#PCA implementation\n\nfrom sklearn.decomposition import PCA\n\npca_reduced=list()\n\nfor j in [25, 50, 100]:\n    pca = PCA(n_components=j)\n    pca_reduced.append(pca.fit_transform(training))","metadata":{"execution":{"iopub.status.busy":"2022-06-24T09:19:15.229082Z","iopub.execute_input":"2022-06-24T09:19:15.229845Z","iopub.status.idle":"2022-06-24T09:19:18.416331Z","shell.execute_reply.started":"2022-06-24T09:19:15.229789Z","shell.execute_reply":"2022-06-24T09:19:18.414412Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#autoencoder implementation\n\nfrom keras.layers import Input, Dense\nfrom keras.models import Model\nfrom sklearn.model_selection import train_test_split\nfrom numpy.random import seed\n\nac_reduced = list()\n\nfor i in [25, 50, 100]:\n    sX = np.asarray(training)\n    ncol = sX.shape[1]\n    X_train=X_test=sX\n    input_dim = Input(shape = (ncol, ))\n\n    # DEFINE THE DIMENSION OF ENCODER ASSUMED i\n    encoding_dim = i\n    # DEFINE THE ENCODER LAYERS\n    encoded1 = Dense(4096/4, activation = 'relu')(input_dim)\n    encoded2 = Dense(encoding_dim, activation = 'relu')(encoded1)\n    # DEFINE THE DECODER LAYERS\n    decoded1 = Dense(4096/4, activation = 'relu')(encoded2)\n    decoded2 = Dense(ncol, activation = 'sigmoid')(decoded1)\n    # COMBINE ENCODER AND DECODER INTO AN AUTOENCODER MODEL\n    autoencoder = Model(inputs = input_dim, outputs = decoded2)\n    # CONFIGURE AND TRAIN THE AUTOENCODER\n    autoencoder.compile(optimizer = 'adadelta', loss = 'binary_crossentropy')\n    # Train Auto Encoder\n    autoencoder.fit(X_train, X_train, epochs = 10, batch_size = 10, shuffle = True, validation_data = (X_test, X_test))\n    # Use Encoder level to reduce dimension of train and test data\n    encoder = Model(inputs = input_dim, outputs = encoded2)\n    # Predict the new data using Encoder\n    encoded_out = encoder.predict(X_test)\n    \n    ac_reduced.append(encoded_out)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T09:19:18.424117Z","iopub.execute_input":"2022-06-24T09:19:18.428962Z","iopub.status.idle":"2022-06-24T09:20:32.912233Z","shell.execute_reply.started":"2022-06-24T09:19:18.428898Z","shell.execute_reply":"2022-06-24T09:20:32.911183Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#purity function implementation\n\nfrom sklearn import metrics\n\ndef purity_score(y_true, y_pred):\n    # compute contingency matrix (also called confusion matrix)\n    contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n    # return purity\n    return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix)\n\n#f1 function implementation\n\ndef f1_measure(y_true , y_pred):\n    return metrics.f1_score(y_true, y_pred, average='micro')\n","metadata":{"execution":{"iopub.status.busy":"2022-06-24T09:20:32.914114Z","iopub.execute_input":"2022-06-24T09:20:32.914583Z","iopub.status.idle":"2022-06-24T09:20:32.922246Z","shell.execute_reply.started":"2022-06-24T09:20:32.914537Z","shell.execute_reply":"2022-06-24T09:20:32.920889Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#Agglomerative Clustering implementation\n\nfrom sklearn.cluster import AgglomerativeClustering\n\nAC = AgglomerativeClustering(n_clusters = 10, affinity = 'euclidean', linkage = 'ward')\n\nprint('Agglomerative Hierarchical Clustering')\n\nprint('PCA')\nfor i,j in zip(pca_reduced, [25,50,100]):\n    labels = AC.fit_predict(i)\n    purity = purity_score(y_true,labels)\n    f1 = f1_measure(y_true,labels)\n    print('M='+str(j)+' purity='+str(purity)+' f1='+str(f1))\n    \nprint('Autoencoder')\nfor i,j in zip(ac_reduced, [25,50,100]):\n    labels = AC.fit_predict(i)\n    purity = purity_score(y_true,labels)\n    f1 = f1_measure(y_true, labels)\n    print('M='+str(j)+' purity='+str(purity)+' f1='+str(f1))","metadata":{"execution":{"iopub.status.busy":"2022-06-24T09:20:32.923625Z","iopub.execute_input":"2022-06-24T09:20:32.924820Z","iopub.status.idle":"2022-06-24T09:20:33.141531Z","shell.execute_reply.started":"2022-06-24T09:20:32.924768Z","shell.execute_reply":"2022-06-24T09:20:33.140155Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from scipy import spatial \nimport math\nK=10 # number of classes\n\n\ndef euclidean_dist(value1, value2):\n    return np.linalg.norm(value1 - value2)\n\n\ndef cosine_dist(value1, value2):\n    return spatial.distance.cosine(value1, value2)\n\n\ndef K_means(X, function, max_iterations):\n    clusters = [[] for _ in range(K)] # we need as many clusters as the classes\n    centroids = []\n    \n    # first we need to initialize the centroids randomly\n    for j in range(K):\n        index = random.randint(0,len(X)-1)\n        centroids.append(X[index])\n   \n    # For each iteration\n    iteration = 0\n    while(iteration<max_iterations):\n        #print(\"Iteration \" + str(iteration))\n        # Creating clusters\n        # for each x value we need to compute the distance with each centroid\n        for x_id, x in enumerate(X):\n            if(function == \"euclidean\"):\n                distances = [euclidean_dist(x, centroids[j]) for j in range(K)]\n            else: # cosine distance\n                distances = [cosine_dist(x, centroids[j]) for j in range(K)]\n            q = np.argmin(distances)\n            clusters[q].append(x_id)\n        \n        # Now we have our clusters ready and we need to compute the new centroids\n        prev_centroids = centroids\n        centroids = [np.mean(cluster, axis = 0) for cluster in clusters]        \n        # We check if the centroids are not altered\n        if(function == \"euclidean\"):\n            differences = [euclidean_dist(prev_centroids[j], centroids[j]) for j in range(K)]\n        else:\n            differences = [cosine_dist(prev_centroids[j], centroids[j]) for j in range(K)]\n\n        sum_dif = sum(differences)\n        #print(\"Sum of differences: \" + str(sum_dif))\n        if sum_dif == 0:\n            break\n        \n        iteration = iteration + 1\n\n    # Now that clusters and centroids are created we will create y_pred\n    y_pred = [[] for _ in range(len(X))]\n    for j, cluster in enumerate(clusters):\n        for i in cluster:\n            y_pred[i] = j\n    return y_pred\n\nprint(\"PCA \\n\")\n# K_means with PCA \nprint(\"Euclidean distance\")\nfor i,j in zip(pca_reduced, [25,50,100]):\n    y_pred = K_means(i,\"euclidean\",100)\n    purity = purity_score(y_true,y_pred)\n    f1 = f1_measure(y_true,y_pred)\n    print('M='+str(j)+' purity='+str(purity)+' f1='+str(f1))\n    \nprint(\"\\nCosine distance\")\nfor i,j in zip(pca_reduced, [25,50,100]):\n    y_pred = K_means(i,\"cosine\",100)\n    purity = purity_score(y_true,y_pred)\n    f1 = f1_measure(y_true,y_pred)\n    print('M='+str(j)+' purity='+str(purity)+' f1='+str(f1))\n    \nprint(\"\\n----------------------------------------------------------------\")\n\n# K_means with Autoencoder \n# Eucleidan distance\nprint(\"Autoencoder \\n\")\nprint(\"Euclidean distance\")\nfor i,j in zip(pca_reduced, [25,50,100]):\n    y_pred = K_means(i,\"euclidean\",100)\n    purity = purity_score(y_true,y_pred)\n    f1 = f1_measure(y_true,y_pred)\n    print('M='+str(j)+' purity='+str(purity)+' f1='+str(f1))\n\nprint(\"\\nCosine distance\")\nfor i,j in zip(pca_reduced, [25,50,100]):\n    y_pred = K_means(i,\"cosine\",100)\n    purity = purity_score(y_true,y_pred)\n    f1 = f1_measure(y_true,y_pred)\n    print('M='+str(j)+' purity='+str(purity)+' f1='+str(f1))","metadata":{"execution":{"iopub.status.busy":"2022-06-24T09:20:33.143312Z","iopub.execute_input":"2022-06-24T09:20:33.143916Z","iopub.status.idle":"2022-06-24T09:21:40.265959Z","shell.execute_reply.started":"2022-06-24T09:20:33.143884Z","shell.execute_reply":"2022-06-24T09:21:40.264777Z"},"trusted":true},"execution_count":9,"outputs":[]}]}